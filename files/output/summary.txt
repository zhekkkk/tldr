The passage discusses a novel neural network architecture called the Transformer, which is designed for sequence transduction tasks like machine translation. Unlike traditional models that rely on recurrent or convolutional neural networks, the Transformer uses only attention mechanisms, making it more parallelizable and faster to train. The model achieves state-of-the-art results on machine translation tasks, scoring 28.4 BLEU on English-to-German and 41.8 BLEU on English-to-French translations. The Transformer also performs well on English constituency parsing tasks. The development of the model was a collaborative effort, with various team members contributing to different aspects of its design and implementation. The work was presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017). The introduction of the passage highlights the established use of recurrent neural networks for sequence modeling and transduction problems, noting recent efforts to improve their computational efficiency.
In this section, the multi-head attention mechanism is detailed, which concatenates outputs from 8 parallel attention layers, each with reduced dimensionality (dk=dv=64) to maintain computational efficiency. The Transformer model uses this multi-head attention in three ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder, with masking to prevent leftward information flow. Additionally, each layer in the encoder and decoder includes a position-wise feed-forward network with two linear transformations and a ReLU activation. The model uses learned embeddings for input and output tokens, sharing weights between embedding layers and the pre-softmax linear transformation. Positional encodings, using sine and cosine functions, are added to input embeddings to provide sequence order information, as the model lacks recurrence and convolution.
The text discusses positional encoding for a model using sinusoidal functions for each dimension, with wavelengths forming a geometric progression. This choice is hypothesized to help the model learn to attend by relative positions. Experiments with learned positional embeddings showed nearly identical results, but the sinusoidal version was chosen for its potential to extrapolate to longer sequences.

The section then compares self-attention layers to recurrent and convolutional layers for sequence transduction tasks, focusing on computational complexity, parallelization, and path length between long-range dependencies. Self-attention layers connect all positions with a constant number of sequential operations and are faster than recurrent layers when sequence length is smaller than the representation dimensionality. For long sequences, self-attention could be restricted to a neighborhood to improve performance. Convolutional layers require stacking to connect all input-output position pairs, increasing path length. Separable convolutions decrease complexity but are still comparable to self-attention combined with a feed-forward layer.

Additionally, self-attention is noted to potentially yield more interpretable models, with attention heads learning different tasks related to sentence structure. The text also mentions plans for further investigation and presents attention distribution examples in the appendix. The following section discusses training.
We used beam search with a beam size of 4 and a length penalty of 0.6, choosing these hyperparameters after experimentation on the development set. The maximum output length during inference was set to the input length plus 50, but terminated early when possible.

Table 2 summarizes our results, comparing translation quality and training costs to other model architectures from the literature. We estimated the number of floating point operations used to train a model by multiplying the training time, the number of GPUs, and an estimate of the sustained single-precision floating-point capacity of each GPU.

To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search but no checkpoint averaging.

In Table 3, rows (A) vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant. While single-head attention performs 0.9 BLEU worse than the best setting, quality also drops with too many heads.

In rows (B), reducing the attention key size hurts model quality, suggesting that determining compatibility is not easy and a more sophisticated compatibility function than dot product may be beneficial. Rows (C) and (D) show that bigger models are better, and dropout is very helpful in avoiding over-fitting. Row (E) replaces our sinusoidal positional encoding with learned positional embeddings, observing nearly identical results to the base model.

To evaluate if the Transformer can generalize to other tasks, we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, about 40K training sentences. We also trained it in a semi-supervised setting, using larger corpora with approximately 17M sentences. We used a vocabulary of 16K tokens for the WSJ only setting and 32K tokens for the semi-supervised setting.

We performed a small number of experiments to select the dropout, both attention and residual, learning rates, and beam size on the Section 22 development set. All other parameters remained unchanged from the English-to-German base translation model.
The text is a list of references from various academic papers and conference proceedings, primarily in the fields of computer vision, natural language processing, and neural networks. The references include details such as authors, titles, publication venues, and years. Some notable works mentioned include "Deep residual learning for image recognition" by Kaiming He et al., "Long short-term memory" by Sepp Hochreiter and JÃ¼rgen Schmidhuber, and "Adam: A method for stochastic optimization" by Diederik Kingma and Jimmy Ba. The list also covers topics like self-training PCFG grammars, neural machine translation, attention mechanisms, and optimization methods in neural networks. The references span a range of years, from 1993 to 2017.